\documentclass[10pt]{extarticle}
\usepackage{graphicx}
\usepackage[margin=.5in]{geometry}
\usepackage{multicol}
\usepackage{helvet}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{subcaption}

\begin{document}
	
	\twocolumn
	
	\title{Controller Assignment}
	\author{Shadman Protik\\Talha Ibn Aziz }
	\date{21/3/2018}
	\maketitle
	
	\section{Problem Statement} \label{prob}
	Let a network be represented by a graph $G=(S,L)$, where $S$ is the set of switches or nodes and $L$ is the list of connecting edges. This network is to be divided into $k$ disjoint sets of switches $S_{i:i=1,2,3...}$ and a controller is to be assigned to each set $S_i$ such that the following four conditions are met:
	\begin{enumerate}
		\item The latency from each controller to the nodes of $S_i$ is minimized.
		\item The latency from each controller to other controllers is minimized.
		\item The reliability of the network is maximized.
		\item The number of controllers, $k$ needs to be minimized as much as possible (cost efficiency). 
	\end{enumerate}

	This is called the Controller Placement Problem.
	Therefore, there are more than one parameters which need to be optimized. Most works\cite{dbcp} defined this problem as a multi-objective combinatorial optimization problem.
	
	Controller Assignment can be divided into two phases:
	\begin{itemize}
		\item Clustering
		\item Controller Selection
	\end{itemize}

	Clustering is the process of determining the number of controllers $k$ and $k$ sets of disjoint switches or 'clusters' such that the target objectives 1 and 4 mentioned in section \ref{prob} are reached. We worked with two well known algorithms DBCP\cite{dbcp} and SPICi\cite{spici} that cluster a network very efficiently to reach different goals. And we proposed a new algorithm Inverse-SPICi which is a variation of SPICi. These three algorithms have been explained in details in sections \ref{algo:dbcp}, \ref{algo:spici} and \ref{algo:ispici} respectively.
	
	Controller selection is the method to select a controller from each cluster that has been formed due to the clustering algorithms. DBCP has used a method of controller selection which has been described in section \ref{dbcp:consel}.
	
	\section{Algorithm 1: DBCP\cite{dbcp}} \label{algo:dbcp}

	\subsection{Clustering} \label{dbcp:cluster}
	DBCP (Density Based Controller Placement) clusters a network using the local density of a node and the minimum distance to a higher density node. The local density of a node $s$ is the count of all the nodes which are at most $d_c$ distance away from $s$ and is denoted by $\rho_s$. The threshold $d_c$ is a distance used to set a limit to the cluster diameter and consequently to find an approximate to the optimal value of $k$.
	\begin{equation}
	\rho_i=\sum_j\chi(d_{ij}-d_c)
	\end{equation}
	
	Here $d_{ij}$ gives the minimum distance between nodes $i$ and $j$. The value of $\chi(x)$ is 1 only for $d_{ij}<d_c$ that is when $x<0$ and is 0 otherwise. Thus $\rho_i$ is the number of nodes that can be reached from node $i$ by traversing at most distance $d_c$.
	
	The minimum distance to a higher density node is represented by the term $\delta_s$ for a node $s$. For the node with highest density there is no other node with higher density and so only for that node, $\delta_s$ holds the distance of the farthest node from $s$.
	
	For a set of nodes $S$ with a set of edges $L$ among them in a graph $G=(S,L)$ they fixed a value of $k$, that is the number of clusters to be formed each with one controller. They formed the clusters and selected a node as controller based on the values of $\rho_s$ and $\delta_s$. The more the values $\rho_s$ and $\delta_s$ for a node $s$, the more it's chances are for being selected as the controller. The method for selecting the cluster head or controller of each sub-network is explained further in detail in the section \ref{dbcp:consel}.
	
	\begin{algorithm}
		\caption{: Density Based Controller Placement}\label{euclid}
		\begin{algorithmic}[1]
			\Procedure{DBCP}{S,L} \\
			$k \gets 0$ \\
			\textbf{for $s$ in S:}
			\State $\rho_s=\sum_{j\in S}\chi(d_{sj}-d_c)$ \\
			\textbf{end for} \\
			\textbf{for $s$ in S:}
			\State $\delta_s=\min_{i:i\in S,p_i>p_s}(d_{is})$ \\
			\textbf{end for} \\
			$\delta \gets \frac{1}{|S|}\sum_{s\in S}\delta_s$ \\
			\textbf{for s in S:}
			\If {$\delta_s>\delta$}	
				\State $k = k + 1$
				\State $s \gets newcluster$
			\Else
				\State $s \gets cluster of nearest higher density$
			\EndIf \\
			\textbf{end for}
			\EndProcedure
		\end{algorithmic}
	\end{algorithm}

	In the given paper \cite{dbcp} they used hop count as the metric for latency. That is all distances were calculated considering node to node distance a constant value of 1 (1 hop). We also considered the case if it is not constant. Therefore we implemented the same algorithm for the case where node to node distance can be any positive real number and used that as the distance metric. This numeric value can represent the bandwidth of that link or channel. It can also represent the transmission rate of the link. We named this new implementation Weighted Density Based Controller Placement or WDBCP and included it in the results analysis and algorithm comparison of section \ref{comp}.
	
	This algorithm has a few drawbacks which are also pointed out with example graphs in the section \ref{comp}.

	\subsection{Controller Selection} \label{dbcp:consel}
	DBCP uses three metrics to determine the controller for a cluster. These three metrics are described in the following sections.
	
	\subsubsection{Controller-to-switch latency}
	For a sub-network graph $S(\theta)$, the average propagation latency for the placement location $v:v\in S(\theta)$ of controller Î¸ is calculated as:
	\begin{equation}	\label{pi1}
	\pi_{avglatency}(S(\theta)) = \min_{v\in S(\theta)} \frac{1}{|S(\theta)|} \sum_{s\in S(\theta)}d(v,s)
	\end{equation}
	
	Thus the average of the distances of the node $v$ from all other nodes is calculated. To select a node as the controller, this value that is $\pi_{avglatency}$ must be minimum.
	
	For the worst case scenario another metric is defined. This metric is denoted by $\pi_{maxlatency}$.
	\begin{equation}	\label{pi2}
	\pi_{maxlatency}(S(\theta)) = \min_{v\in S(\theta)} \max_{s\in S(\theta)}d(v,s)
	\end{equation}
	
	This is the maximum of the distances of the node $v$ from all the other nodes $s:s\in S(\theta)$ in the cluster $S(\theta)$. This maximum must be minimized for a cluster. Therefore all nodes are considered when calculating $\pi_{maxlatency}$ and only the switch with the minimum value is selected as controller.
	
	\subsubsection{Inter-controller latency}
	The inter controller latency must be reduced as much as possible when selecting controller. But the controller to controller distance cannot be determined when the controllers are yet to be selected. Thus a new metric is used which is denoted by $\pi_{inter\_controller}$.
	\begin{equation}	\label{pi3}
	\pi_{inter\_controller}(S(\theta)) = \min_{v\in S(\theta)} \frac{1}{|S|} \sum_{s\in (S-S(\theta))}d(v,s)
	\end{equation}
	
	Thus it is the sum of the distances of the node $v$ of the cluster $S(\theta)$ to all the nodes of other clusters divided by the total number of nodes in the network. The node which has the minimum value of $\pi_{inter\_controller}$ is selected as the controller $\theta$.
	
	\subsubsection{Latency}
	This is the final metric which is the sum of all three previous metrics in equations (\ref{pi1}), (\ref{pi2}) and (\ref{pi3}).
	\begin{equation}
	\pi_{latency} = \pi_{avglatency} + \pi_{maxlatency} + \pi_{inter\_controller}
	\end{equation}	\label{pi4}
	\[\pi_{latency}S(\theta)=\min_{v\in S(\theta)}(\frac{1}{|S(\theta)|} \sum_{s\in S(\theta)}d(v,s) + \max_{s\in S(\theta)}d(v,s)\]
	
	\begin{equation}
	 + \frac{1}{|S|} \sum_{s\in (S-S(\theta))}d(v,s))
	\end{equation}
	Thus for each node in a cluster $S(\theta)$ we find the value of $\pi_{latency}(S(\theta))$ and then we select the node with minimum $\pi_{latency}$ as the controller $\theta$ of the cluster.
	
	\subsection{Drawbacks}
	There are quite a few drawbacks of this network clustering algorithm. These are described in brief as follows:
	
	\begin{enumerate}
		\item For a densely connected graph almost every other node can be reached from a particular node in 1 or 2 hops. As such the only possible values of constant $d_c$ are 1 or 2. For the value of constant $d_c=2$ all the nodes have the same density and therefore average minimum distance to higher density nodes is equal to minimum distance to higher density node for all the nodes and so according to the algorithm $k=0$ which basically means there are no clusters, which is not possible.
		\item For $d_c$ equal to 1 we are just basically calculating density based on incident degree of a node. Thus only 2 minimum distances to higher density nodes are possible which are 1 and 2.
		There are only two possible results in this case, one is $k=0$ as mentioned in previous point and the other is a certain number of controllers. Therefore there is very less variation in the number of clusters.
		\item The Algorithm selects the cluster for a node following the condition of line 15 of algorithm \ref{algo:dbcp}. That is an unclustered node is assigned a cluster based on the cluster of it's nearest higher density. This creates a problem if a loop occurs such that there are two switches $s_1$ and $s_2$, and $s_1$ has nearest higher density $s_2$ and $s_2$ has nearest higher density $s_1$. This is possible when one switch is of highest density and other is farthest from the switch with highest density which is quite often in the case of dense graphs.
	\end{enumerate}

	We showed the drawbacks using different scenarios again in section \ref{comp}.
	
	\section{Algorithm 2: SPICi}	\label{algo:spici}
	
	SPICi ('spicy', Speed and Performance In Clustering) clusters a connected undirected graph $G=(V,E)$ with edges that have confidence values of the continuous range $(0,1)$ so that each cluster has a seed as center from which clustering starts. These edges can be represented by $w_{u,v}$ such that $u,v\in V$, $w_{u,v}\in E$ and $0\le w_{u,v}\le1$. It works with three variables and two thresholds. The three variables can be defined as the weighted degree of a node $d_w$, the density of a set of nodes $S$ and the support of a node $u\in V$ with respect to a set of nodes $S$:
	\begin{equation}
	d_w(u)_{u\in V} = \sum_{v:v\in V,(u,v) \in E}w_{u,v}
	\end{equation}
	
	Therefore $d_w(u)_{u\in V}$ is the sum of all the weights of the edges that connect $u$ with any other adjacent node $v$ of the graph $G=(V,E)$.
	
	\begin{equation}
	density(S) = \frac{\sum_{(u,v)\in E}w_{u,v}}{|S|*(|S|-1)/2} 
	\end{equation}
	
	In other words, $density(S)$ is the sum of the edges that connect every node $u$ with every other node $v$ of $S$ divided by the number of total possible nodes that is $|S|*(|S|-1)/2$ where $|S|$ is the number of nodes present in the set of nodes $S$.
	
	\begin{equation} \label{support}
	support(u,S) = \sum_{v\in S} w_{u,v}
	\end{equation}
	And $support(u,S)$ is the sum of the edges that connect a node $u$ with the nodes adjacent to it present in the set of nodes $S$.
	
	The thresholds are: $T_s$ which determines whether a node is to be included in the cluster based on the cluster size and the connectivity of the node to the cluster and: $T_d$ which includes a node to the cluster based on the density increased when the node is added.
	\begin{algorithm}
		\caption{: SPICi}\label{spicicode}
		\begin{algorithmic}[1]
			\Procedure{Search}{V,E} \\
			\textbf{Initialize} $DegreeQ = V$ \\
			\textbf{While} $DegreeQ \neq empty$
			\State Extract u from DegreeQ with largest $d_w(u)$
			\If {there is $v_{v:v,u\in E}\in DegreeQ$}
			\State $v \gets secondseed(DegreeQ,E,u)$
			\If {$v\neq null$} $S \gets Expand(u,v)$
			\EndIf
			\Else 
				\State $S \gets \{u\}$
			\EndIf
			\State $V \gets V - S $
			\State $Degree Q \gets Degree Q - S$
			\State $d_w(t)_{t:t\in DegreeQ,(t,s)_{s\in S}\in E} = d_w(t) - support(t,S)$
			\EndProcedure\\
			\Procedure{SecondSeed}{V,E,u} \\
			$bin[i]_{i:i=(1,5)} \gets s_{s:s\in V,(s,u)\in E}$ \\
			\textbf{for i = 1 to 5}
			\If {$bin[i] \neq empty$}
				\State \Return $v$ \textbf{if} $d_w(v)=\max_{s:s\in bin[i]}{d_w(s)}$
			\EndIf\\
			\Return $null$
			\EndProcedure\\
			\Procedure{Expand}{V,E,u,v}\\
			\textbf{Initialize} the cluster $S \gets \{u,v\}$ \\
			\textbf{Initialize} $CandidateQ = S_{S:s\in S,(s,u),(s,v)\in E}$\\
			\textbf{While} $CandidateQ \neq empty$
			\State Extract $t$ from Candidate with highest $support(t,S)$
			\If {$support(t,S)\geq T_s*|S|*density(S)$ and
				$density(S + t)> T_d$}
				\State $S\gets S+\{t\}$
				\State $CanditateQ \gets CandidateQ + \{s_{s:(s,t)\in E}\}$
				\State $CandidateQ \gets CandidateQ - \{s_{s:s\not\in CandidateQ}\}$
				\Else
				\State break from loop
				\EndIf \\
				\Return S
			\EndProcedure
		\end{algorithmic}
	\end{algorithm}
	The nodes are added to bins 1,2,3,4 and 5 respectively if their connected edge value $w_{s,u}\leq0.2,0.3,0.4$ and $0.5$ such that $s,u\in V$ and $(s,u)\in E$.
	
	
	\section{Proposed 1: I-SPICi} \label{algo:ispici}
	
	The algorithm I-SPICi or Inverse-SPICi is a derivative of SPICi and one of our proposed algorithms. It has two steps \emph{Clustering} and \emph{Controller Selection} which are given as follows.
	
	\subsection{Clustering}
	This algorithm is taken from SPICi without any post processing or modification except that the cost is taken in an inverse manner. If the graph is represented by $G=(V,E)$ where $V$ is the set of nodes and $E$ is the set of edges containing edge costs $w_{u,v}$ such that $u,v\in V$ and $(u,v)\in E$ the costs can be expressed by the following equations.
	\begin{equation}
	max\_cost = \max_{u,v\in E}(w_{u,v}+1)
	\end{equation}
	\begin{equation}
	cost_{u,v} = max\_cost - w_{u,v}
	\end{equation}
	
	Here $max\_cost$ is the maximum edge distance or cost of all the edges present in the graph. To each edge cost, 1 is added before calculating the maximum cost so that the selected maximum cost ($max\_cost$) is greater than the actual maximum cost. Therefore after subtracting each of the edge costs from this maximum cost will not result in zero even for the actual maximum cost. When all the costs have been reassigned, this results in the costs being reversed for any pair of adjacent nodes $u,v:(u,v)\in E$. 
	
	Thus the highest edge cost becomes the lowest and the lowest edge cost becomes the highest.
	This cost acts as the new edge value when calculating the values of weighted degree, support and density of the required set of nodes.
	
	\subsection{Controller Selection}
	We selected the controller for a cluster by calculating the $support(u,S)$ (as expressed in equation (\ref{support})) for a set of nodes $S$ which in this case is the cluster in consideration and a node $u:u\in S$. Each node gives a value of support and we selected the node with the maximum support. This can be improved further if we again take the original edge costs instead of the reversed ones or if we use the same controller selection method as used in DBCP.
	
	\section{Proposed 2: G-SPICi} \label{algo:gspici}
	
	\subsection{Clustering}
	This algorithm is  also taken from SPICi without any post processing. The only modification is that all the nodes of a cluster except the \textbf{first seed protein} (as per stated in \cite{spici}) or \textbf{cluster head} (in terms of clustering terminology), are selected using the same method. The node with maximum support with respect to the current cluster that is being built is selected each time. Therefore there would be no second seed selection function. The pseudo-code would also be changed. The changed pseudo-code is given below in algorithm \ref{gspicicode}.
	\begin{algorithm}
		\caption{:Greedy-SPICi}\label{gspicicode}
		\begin{algorithmic}[1]
			\Procedure{Search}{V,E} \\
			\textbf{Initialize} $DegreeQ = V$ \\
			\textbf{While} $DegreeQ \neq empty$
			\State Extract u from DegreeQ with largest $d_w(u)$
			\State $S \gets Expand(u)$
			\State $V \gets V - S $
			\State $Degree Q \gets Degree Q - S$
			\State $d_w(t)_{t:t\in DegreeQ,(t,s)_{s\in S}\in E} = d_w(t) - support(t,S)$
			\EndProcedure\\
			\Procedure{Expand}{V,E,u}\\
			\textbf{Initialize} the cluster $S \gets \{u\}$ \\
			\textbf{Initialize} $CandidateQ = S_{S:s\in S,(s,u)\in E}$\\
			\textbf{While} $CandidateQ \neq empty$
			\State Extract $t$ from Candidate with highest $support(t,S)$
			\If {$support(t,S)\geq T_s*|S|*density(S)$ and
				$density(S + t)> T_d$}
			\State $S\gets S+\{t\}$
			\State $CanditateQ \gets CandidateQ + \{s_{s:(s,t)\in E}\}$
			\State $CandidateQ \gets CandidateQ - \{s_{s:s\not\in CandidateQ}\}$
			\Else
			\State break from loop
			\EndIf \\
			\Return S
			\EndProcedure
		\end{algorithmic}
	\end{algorithm}

	This algorithm performs better than SPICi because SPICi selects the second seed in such a way that it has the maximum distance from first seed protein. This does not give good result when there are link with greater latencies. SPICi tends to select both nodes connected by the larger edge for the same cluster. Greedy SPICi or G-SPICi selects the one with maximum support thus this problem is somewhat reduced.
	
	\subsection{Controller Selection}
	We selected the controller for a cluster by calculating the $support(u,S)$ (as expressed in equation (\ref{support})) for a set of nodes $S$ which in this case is the cluster in consideration and a node $u:u\in S$. Each node gives a value of support and we selected the node with the maximum support. This can be improved further if we again take the original edge costs instead of the reversed ones or if we use the same controller selection method as used in DBCP.
	
	\section{Thresholds $T_s$ and $d_c$}
	In SPICi the value of Tc have an effect on the cluster size. The Greater the value, less number of nodes there are in a cluster. The value was originally set to 0.5 in SPICi.
	But as we experimented more, we think the value depends more on the type of input graph rather than being a fixed value. For SPICi we tried to take the value near 0.5.
	
	For Inverse-SPICi when the value is near 1 it gives better results. Slight variation in the value may change the cluster size dramatically. 
	
	\textbf{Problem:} Determining the value of $T_s$
	
	
	\textbf{Solution:} We can do a Binary/Ternary search.
	
	\textbf{Procedure:} 
	\begin{enumerate}
		\item First We will select a value of K (The number of cluster). We won't have more than K clusters.
		\item Perform a Search to determine the value of $T_s$ depending on the result that SPICi gives. We can find a suitable metric for this.
	\end{enumerate}
	
	
	 
	\section{Performance Evaluation}
	
	Initially we defined the metrics that we used to evaluate the performance of the algorithms. Then we compared the algorithms using those metrics.
	
	\subsection{Performance Metrics 1} \label{perfm}
	
	To evaluate the performance of the existing algorithms and the proposed algorithms, we used a metric called $\pi$ to define how well an algorithm is compared to another. We defined most of this metric based on the cluster head selection parameters of the algorithm DBCP. It is the sum of three values which are defined in the equations (\ref{pi1}), (\ref{pi2}) and (\ref{pi3}). Therefore these three variables are the three metrics used to select a controller in the cluster. And so each node in the network will have separate values for these three metric. But we changed the third metric or $\pi_{inter\_controller}$ to the changed metric $\pi_{ic}(S\theta)$ which can be expressed as follows:
	\begin{equation}
	\pi_{ic}(S(\theta)) = \min_{v\in S(\theta)} \frac{1}{|S|-S(\theta)} \sum_{s\in (S-S(\theta))}d(v,s)
	\end{equation}
	
	Here $|S|$ is the total number of nodes in the network and $S(\theta)$ is the number of nodes in the cluster or sub-network that is being considered.
	
	Each cluster or controller will have separate values for these three metrics. And the sum of these three metrics will give as the final metric. And as such their sum will also be separate for each node. We will calculate all these 3 values and their sum for each controller. So these values would be $\pi_{avglatency}(S(\theta))$, $\pi_{maxlatency}(S(\theta))$ and $\pi_{ic}(S(\theta))$ and their sum $\pi_{latency}(S(\theta))$ for each controller $S(\theta)$. Thus the values of these metrics for the whole network is the sum of all these controller specific values in the following way.
	
	\begin{equation}
	\pi_{avg} = \sum_{S(\theta)\in S}(\pi_{avglatency}(S(\theta)))
	\end{equation}
	\begin{equation}
	\pi_{max} = \sum_{S(\theta)\in S}(\pi_{maxlatency}(S(\theta)))
	\end{equation}
	\begin{equation}
	\pi_{ic} = \sum_{S(\theta)\in S}(\pi_{ic}(S(\theta)))
	\end{equation}
	\begin{equation}	\label{final}
	\pi = \sum_{S(\theta)\in S}(\pi_{latency}(S(\theta)))
	\end{equation}
	
	The final performance metric is $\pi$ as given in equation (\ref{final}) for a whole network. We compared the algorithms primarily based on $\pi$. For exapmle, for a definite value of $k$ that is the number of controllers, if $\pi$ of algorithm 1 is less than that of algorithm 2 then algorithm 1 is better than algorithm 2 for that $k$.
	
	\subsection{Performance Metrics 2} \label{myperfm}
	We have used a metric to evaluate the results of an algorithm and to compare an algorithm with another. We call this metric $\pi_{latency}$ of a clustered network. It is the sum of two variables which are called $\pi_{avglatency}$ and $\pi_{intercontroller}$. They can be expressed as the following two equations.
	
	\begin{equation}
	\pi_{avglatency}=\frac{\sum_{S\in V}\sum_{c,i\in S}d_{ci}}{k}
	\end{equation}
	
	Here $S$ is the set of nodes or cluster of nodes that belong to the nodes ($V$) of the graph ($G=(V,E)$). For each cluster $S$ the controller is the node $c$ and $i$ any other node. Therefore $d_{ci}$ is any distance from a controller to any of the nodes of the cluster. Ank $k$ is the number of clusters created or in other words controllers.
	
	\begin{equation}
	\pi_{intercontroller}=\frac{\sum_{i,j\in V}d_{ij}}{k}
	\end{equation}
	Here $i$ and $j$ are controllers of the clustered network. Therefore $\pi_{intercontroller}$ is the sum of all possible controller to controller distance divided by the number of controllers or number of clusters. It is the average of the sum of distances of each controller to all other controllers.
	
	\subsection{Comparison of Algorithms} \label{comp}
	
	We compared the different algorithms using the explained metrics. We prepared three scenarios for each algorithm. And then we compared the algorithms for several random scenarios. In each scenario we showed the comparisons in the form of a graph for each possible values of k. Using these figures (for different scenarios) we can observe the performance of the algorithms for different values of k and compare them for each k.
	
	For I-SPICi pr Inverse-SPICi the clusters are formed before while selecting the value of k, that is the number of controllers. Thus it is not possible to cluster a network using I-SPICi for a specific value of k. We tried all possible values of the thresholds $T_s$ and $T_d$ to get a number of results with different values of $k$. For each matching value of k (duplicate $k$ values) we considered the result only with the lowest value of $\pi$ (latency). Then we plotted the results in comparison with other algorithms.
	\subsubsection{Scenario 1}
	Network Graph Description:
	\begin{itemize}
		\item Nodes: 34
		\item Edges: 42
		\item Cycles or Loops?: Yes
		\item Attributes: Unweighted, Sparse
	\end{itemize}
	This is the dataset of OS3E, a real network present all over Canada and The United States that is used for experimental purposes. It is a good example for a big network graph having nodes with greater connectivity. The graph is unweighted so all weights are considered 1 for cases of using algorithms that are for weighted graphs.
	\begin{figure}
		\includegraphics[width=\linewidth]{OS3E.png}
		\caption{Figure for Scenario 1}
		\label{fig:os3e}
	\end{figure}
	\begin{figure*}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{dbcp_1.png}
			\caption{DBCP for Scenario 1}
			\label{fig:dbcp1}
		\end{subfigure}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{wdbcp_1.png}
			\caption{WDBCP for Scenario 1}
			\label{fig:wdbcp1}
		\end{subfigure}
		\vskip \baselineskip
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{gspici_1.png}
			\caption{Greedy-SPICi for Scenario 1}
			\label{fig:gspici1}
		\end{subfigure}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{ispici_1.png}
			\caption{Inverse-SPICi for Scenario 1}
			\label{fig:ispici1}
		\end{subfigure}
	\end{figure*}
	\begin{figure*}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{comparison_1.png}
			\caption{Comparison for Scenario 1 using Metric 1}
			\label{fig:comp1}
		\end{subfigure}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{our_comp_1.png}
			\caption{Comparison for Scenario 1 using Metric 2}
			\label{fig:ourcomp1}
		\end{subfigure}
	\end{figure*}
	From figure \ref{fig:comp1} we can see that the latency ($\pi$) of I-SPICi suddenly rises for the values of k from 1 to 5 and the suddenly decreases and then gradually keeps increasing. This is beacuse for a smaller than optimal number of clusters SPICi creates less clusters and thus unequal clusters, as a result nodes are assigned haphazardly which gives very high latency values. When the controller number is more it performs comparatively better as it can select seed proteins efficiently so that the start nodes for clustering are selected with appropriate distances from each other.
	
	\subsubsection{Scenario 2}
	Network Graph Description:
	\begin{itemize}
		\item Nodes: 10
		\item Edges: 11
		\item Cycles or Loops?: Yes
		\item Attributes: Weighted, Sparse
	\end{itemize}
	This is the SPICi dataset containing less nodes with greater proximity and similar weight edges. The edge weights are in the continuous range $(0,1)$. It is shown in the figure \ref{fig:spici}.
	\begin{figure}
		\includegraphics[width=\linewidth]{Spici.png}
		\caption{Figure for Scenario 2}
		\label{fig:spici}
	\end{figure}
	\begin{figure*}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{dbcp_2.png}
			\caption{DBCP for Scenario 2}
			\label{fig:dbcp2}
		\end{subfigure}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{wdbcp_2.png}
			\caption{WDBCP for Scenario 2}
			\label{fig:wdbcp2}
		\end{subfigure}
		\vskip \baselineskip
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{gspici_2.png}
			\caption{Greedy-SPICi for Scenario 2}
			\label{fig:gspici2}
		\end{subfigure}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{ispici_2.png}
			\caption{Inverse-SPICi for Scenario 2}
			\label{fig:ispici2}
		\end{subfigure}
	\end{figure*}
	\begin{figure*}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{comparison_2.png}
			\caption{Comparison for Scenario 2 using Metric 1}
			\label{fig:comp2}
		\end{subfigure}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{our_comp_2.png}
			\caption{Comparison for Scenario 2 using Metric 2}
			\label{fig:ourcomp2}
		\end{subfigure}
	\end{figure*}
	In this scenario also I-SPICi gives a higher result for smaller number of controllers and better results for larger values of $k$ (number of controllers). We can understand from the figure \ref{fig:comp2} that there is an optimal point where I-SPICi gives a better result for a relatively lesser number of controllers.
	
	
	\subsubsection{Scenario 3}
	Network Graph Description:
	\begin{itemize}
		\item Nodes: 15
		\item Edges: 18
		\item Cycles or Loops?: Yes
		\item Attributes: Weighted, Sparse
	\end{itemize}
	This is a dataset we created with nodes at greater distances and a few nodes creating closely connected zones. The graph is depicted in figure \ref{fig:separate}.
	\begin{figure}
		\includegraphics[width=\linewidth]{separated.png}
		\caption{Figure for Scenario 3}
		\label{fig:separate}
	\end{figure}
	\begin{figure*}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{dbcp_3.png}
			\caption{DBCP for Scenario 3}
			\label{fig:dbcp3}
		\end{subfigure}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{wdbcp_3.png}
			\caption{WDBCP for Scenario 3}
			\label{fig:wdbcp3}
		\end{subfigure}
		\vskip \baselineskip
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{gspici_3.png}
			\caption{Greedy-SPICi for Scenario 3}
			\label{fig:gspici3}
		\end{subfigure}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{ispici_3.png}
			\caption{Inverse-SPICi for Scenario 3}
			\label{fig:ispici3}
		\end{subfigure}
	\end{figure*}
	\begin{figure*}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{comparison_3.png}
			\caption{Comparison for Scenario 3 using Metric 1}
			\label{fig:comp3}
		\end{subfigure}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{our_comp_3.png}
			\caption{Comparison for Scenario 3 using Metric 2}
			\label{fig:ourcomp3}
		\end{subfigure}
	\end{figure*}
	I-SPICi gives better results for k values greater than and equal to 4 as we can see from the figure \ref{fig:comp3}. Which gives us an idea about what the optimal value of $k$ might be.
	
	\subsubsection{Scenario 4}
	
	Network Graph Description:
	\begin{itemize}
		\item Nodes: 100
		\item Edges: 2907
		\item Cycles or Loops?: Yes
		\item Attributes: Weighted, Very Dense
	\end{itemize}
	\begin{figure*}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{wdbcp_4.png}
			\caption{WDBCP for Scenario 4}
			\label{fig:wdbcp4}
		\end{subfigure}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{gspici_4.png}
			\caption{Greedy-SPICi for Scenario 4}
			\label{fig:gspici4}
		\end{subfigure}
		\vskip \baselineskip
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{ispici_4.png}
			\caption{Inverse-SPICi for Scenario 4}
			\label{fig:ispici4}
		\end{subfigure}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{comparison_4.png}
			\caption{Comparison for Scenario 4 using Metric 1}
			\label{fig:comp4}
		\end{subfigure}
	\end{figure*}
	\begin{figure}
			\includegraphics[width=\linewidth]{our_comp_4.png}
			\caption{Comparison for Scenario 4 using Metric 2}
			\label{fig:ourcomp4}
	\end{figure}
	
	\subsubsection{Scenario 5}
	
	Network Graph Description:
	\begin{itemize}
		\item Nodes: 100
		\item Edges: 153
		\item Cycles or Loops?: Yes
		\item Attributes: Weighted, Sparse
	\end{itemize}
	\begin{figure*}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{dbcp_5.png}
			\caption{DBCP for Scenario 5}
			\label{fig:dbcp5}
		\end{subfigure}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{wdbcp_5.png}
			\caption{WDBCP for Scenario 5}
			\label{fig:wdbcp5}
		\end{subfigure}
		\vskip \baselineskip
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{gspici_5.png}
			\caption{Greedy-SPICi for Scenario 5}
			\label{fig:gspici5}
		\end{subfigure}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{ispici_5.png}
			\caption{Inverse-SPICi for Scenario 5}
			\label{fig:ispici5}
		\end{subfigure}
	\end{figure*}
	\begin{figure*}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{comparison_5.png}
			\caption{Comparison for Scenario 5 using Metric 1}
			\label{fig:comp5}
		\end{subfigure}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{our_comp_5.png}
			\caption{Comparison for Scenario 5 using Metric 2}
			\label{fig:ourcomp5}
		\end{subfigure}
	\end{figure*}
	
	\subsubsection{Scenario 6}
	
	Network Graph Description:
	\begin{itemize}
		\item Nodes: 100
		\item Edges: 350
		\item Cycles or Loops?: Yes
		\item Attributes: Weighted, Slightly Dense
	\end{itemize}
	\begin{figure*}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{dbcp_6.png}
			\caption{DBCP for Scenario 6}
			\label{fig:dbcp6}
		\end{subfigure}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{wdbcp_6.png}
			\caption{WDBCP for Scenario 6}
			\label{fig:wdbcp6}
		\end{subfigure}
		\vskip \baselineskip
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{gspici_6.png}
			\caption{Greedy-SPICi for Scenario 6}
			\label{fig:gspici6}
		\end{subfigure}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{ispici_6.png}
			\caption{Inverse-SPICi for Scenario 6}
			\label{fig:ispici6}
		\end{subfigure}
	\end{figure*}
	\begin{figure*}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{comparison_6.png}
			\caption{Comparison for Scenario 6 using Metric 1}
			\label{fig:comp6}
		\end{subfigure}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{our_comp_6.png}
			\caption{Comparison for Scenario 6 using Metric 2}
			\label{fig:ourcomp6}
		\end{subfigure}
	\end{figure*}
	
	\subsubsection{Scenario 7}
	
	Network Graph Description:
	\begin{itemize}
		\item Nodes: 100
		\item Edges: 1028
		\item Cycles or Loops?: Yes
		\item Attributes: Weighted, Moderately Dense
	\end{itemize}
	
	\begin{figure*}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{wdbcp_7.png}
			\caption{WDBCP for Scenario 7}
			\label{fig:wdbcp7}
		\end{subfigure}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{gspici_7.png}
			\caption{Greedy-SPICi for Scenario 7}
			\label{fig:gspici7}
		\end{subfigure}
		\vskip \baselineskip
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{ispici_7.png}
			\caption{Inverse-SPICi for Scenario 7}
			\label{fig:ispici7}
		\end{subfigure}
		\begin{subfigure}{0.5\linewidth}
			\includegraphics[width=\linewidth]{comparison_7.png}
			\caption{Comparison for Scenario 7 using Metric 1}
			\label{fig:comp7}
		\end{subfigure}
	\end{figure*}
	\begin{figure}
		\includegraphics[width=\linewidth]{our_comp_7.png}
		\caption{Comparison for Scenario 7 using Metric 2}
		\label{fig:ourcomp7}
	\end{figure}

	\bibliographystyle{plain}
	\bibliography{references}
\end{document}